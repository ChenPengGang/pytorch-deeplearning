{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0102e-38, 1.0286e-38, 1.0194e-38],\n",
      "        [9.6429e-39, 9.2755e-39, 9.1837e-39],\n",
      "        [9.3674e-39, 1.0745e-38, 1.0653e-38],\n",
      "        [9.5510e-39, 1.0561e-38, 1.0194e-38],\n",
      "        [1.1112e-38, 1.0561e-38, 9.9184e-39]])\n"
     ]
    }
   ],
   "source": [
    "#创建未初始化的Tensor\n",
    "x=torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6589, 0.4637, 0.6796],\n",
      "        [0.8463, 0.8002, 0.4982],\n",
      "        [0.1946, 0.1563, 0.1627],\n",
      "        [0.5215, 0.1949, 0.4019],\n",
      "        [0.1248, 0.4759, 0.9565]])\n"
     ]
    }
   ],
   "source": [
    "#随机初始化\n",
    "x=torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "#Long型全0的Tensor\n",
    "x=torch.zeros(5,3,dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.0000, 3.0000, 5.5000])\n"
     ]
    }
   ],
   "source": [
    "#根据数据创建\n",
    "x=torch.tensor([5,3,5.5])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-2.5872, -1.8133,  1.6661],\n",
      "        [-0.6300, -1.2444,  0.5798],\n",
      "        [ 0.9687, -0.3886,  1.6252],\n",
      "        [ 0.3000,  0.0670, -0.4707],\n",
      "        [-0.9366,  0.7646,  1.1399]])\n"
     ]
    }
   ],
   "source": [
    "#还可以通过现有的Tensor来创建，此方法会默认重用输入Tensor的一些属性，例如数据类型，除非自定义数据类型。\n",
    "x=x.new_ones(5,3,dtype=torch.float64)\n",
    "print(x)\n",
    "x=torch.randn_like(x,dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "#Tensor的形状\n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2844, -1.4056,  2.4371],\n",
      "        [ 0.2555, -0.8432,  0.8808],\n",
      "        [ 1.8159,  0.2977,  1.8836],\n",
      "        [ 0.6483,  0.9414,  0.1167],\n",
      "        [-0.6311,  1.6994,  1.9107]])\n"
     ]
    }
   ],
   "source": [
    "#加法\n",
    "y=torch.rand(5,3)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2844, -1.4056,  2.4371],\n",
      "        [ 0.2555, -0.8432,  0.8808],\n",
      "        [ 1.8159,  0.2977,  1.8836],\n",
      "        [ 0.6483,  0.9414,  0.1167],\n",
      "        [-0.6311,  1.6994,  1.9107]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2844, -1.4056,  2.4371],\n",
      "        [ 0.2555, -0.8432,  0.8808],\n",
      "        [ 1.8159,  0.2977,  1.8836],\n",
      "        [ 0.6483,  0.9414,  0.1167],\n",
      "        [-0.6311,  1.6994,  1.9107]])\n"
     ]
    }
   ],
   "source": [
    "#指定输出\n",
    "result=torch.empty(5,3)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2844, -1.4056,  2.4371],\n",
      "        [ 0.2555, -0.8432,  0.8808],\n",
      "        [ 1.8159,  0.2977,  1.8836],\n",
      "        [ 0.6483,  0.9414,  0.1167],\n",
      "        [-0.6311,  1.6994,  1.9107]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 我们还可以使用类似NumPy的索引操作来访问Tensor的一部分，需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5872, -0.8133,  2.6661])\n",
      "tensor([-1.5872, -0.8133,  2.6661])\n"
     ]
    }
   ],
   "source": [
    "y=x[0,:]\n",
    "y+=1\n",
    "print(y)\n",
    "print(x[0,:]) #源tensor也被改了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改变形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "y=x.view(15)\n",
    "z=x.view(-1,5)\n",
    "print(x.size(),y.size(),z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5872,  0.1867,  3.6661],\n",
      "        [ 0.3700, -0.2444,  1.5798],\n",
      "        [ 1.9687,  0.6114,  2.6252],\n",
      "        [ 1.3000,  1.0670,  0.5293],\n",
      "        [ 0.0634,  1.7646,  2.1399]])\n",
      "tensor([-0.5872,  0.1867,  3.6661,  0.3700, -0.2444,  1.5798,  1.9687,  0.6114,\n",
      "         2.6252,  1.3000,  1.0670,  0.5293,  0.0634,  1.7646,  2.1399])\n"
     ]
    }
   ],
   "source": [
    "x+=1\n",
    "print(x)\n",
    "print(y) #也加了1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5872, -1.8133,  1.6661],\n",
      "        [-1.6300, -2.2444, -0.4202],\n",
      "        [-0.0313, -1.3886,  0.6252],\n",
      "        [-0.7000, -0.9330, -1.4707],\n",
      "        [-1.9366, -0.2354,  0.1399]])\n",
      "tensor([-1.5872, -0.8133,  2.6661, -0.6300, -1.2444,  0.5798,  0.9687, -0.3886,\n",
      "         1.6252,  0.3000,  0.0670, -0.4707, -0.9366,  0.7646,  1.1399])\n"
     ]
    }
   ],
   "source": [
    "x_cp=x.clone().view(15)\n",
    "x-=1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6874])\n",
      "0.6874387264251709\n"
     ]
    }
   ],
   "source": [
    "#另外一个常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number\n",
    "x=torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "#由于x和y分别是1行2列和3行1列的矩阵，如果要计算x + y，那么x中第一行的2个元素被广播（复制）到了第二行和第三行，而y中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。\n",
    "x=torch.arange(1,3).view(1,2)\n",
    "print(x)\n",
    "y=torch.arange(1,4).view(3,1)\n",
    "print(y)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运算的内存开销"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 前面说了，索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。为了演示这一点，我们可以使用Python自带的id函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2])\n",
    "y=torch.tensor([3,4])\n",
    "id_before=id(y)\n",
    "y=y+x\n",
    "print(id(y)==id_before)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#如果想指定结果到原来的y的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把x + y的结果通过[:]写进y对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2])\n",
    "y=torch.tensor([3,4])\n",
    "id_before=id(y)\n",
    "y[:]=y+x\n",
    "print(id(y)==id_before)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 我们还可以使用运算符全名函数中的out参数或者自加运算符+=(也即add_())达到上述效果，例如torch.add(x, y, out=y)和y += x(y.add_(x))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2])\n",
    "y=torch.tensor([3,4])\n",
    "id_before=id(y)\n",
    "torch.add(x,y,out=y)\n",
    "print(id(y)==id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor和NumPy相互转换"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#我们很容易用numpy()和from_numpy()将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(5)\n",
    "b=a.numpy()\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "b+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy数组转Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a=np.ones(5)\n",
    "b=torch.from_numpy(a)\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "b+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c=torch.tensor(a)\n",
    "a+=1\n",
    "print(a,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3], device='cuda:0')\n",
      "tensor([2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
